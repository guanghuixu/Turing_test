mention pair mention pair models that predict whether or not two mentions are coreferent have historically been very effective for coref-erence resolution , but do not make use of entity-level information . however , we show that the scores produced by such mention pair models can be aggregated to define powerful entity-level features between clusters of mentions . using these features , we train an entity-centric coreference system that learns an effective policy for building up coreference chains incrementally . the mention pair scores are also used to prune the search space the entity-centric coreference system works in , allowing for efficient training with an exact loss function . we evaluate our entity-centric coreference system on the english portion of the 2012 conll shared task dataset and show that entity-centric coreference system improves over the current state of the art .
we address the problem of segmenting and recognizing objects in real world images , focusing on challenging articulated categories such as humans and other animals . for this purpose , we propose a novel design for region-based object detectors that integrates efficiently top-down information from scanning-windows part models and global appearance cues . our detectors produce class-specific scores for bottom-up regions , and then aggregate the votes of multiple overlapping candidates through pixel classification . we evaluate our approach on the pascal segmentation challenge , and report competitive performance with respect to current leading techniques . on voc2010 , our method obtains the best results in 6/20 categories and the highest performance on articulated objects .
the electric network frequency signal can be captured in multimedia recordings due to electromagnetic influences from the power grid at the time of recording . recent work has exploited the enf signals for forensic applications , such as authenticating and detecting forgery of enf-containing mul-timedia signals , and inferring their time and location of creation . in this paper , we explore a new potential of enf signals for automatic synchronization of audio and video . the enf signals as a time-varying random process can be used as a timing fingerprint of multimedia signals . synchronization of audio and video recordings can be achieved by aligning their embedded enf signals . we demonstrate the proposed scheme with two applications : multi-view video synchronization and synchronization of historical audio recordings . the experimental results show the enf based synchronization approach is effective , and has the potential to solve problems that are intractable by other existing methods .
we report an investigation of the perception of american english phonemes by dutch listeners proficient in english . listeners identified either the consonant or the vowel in most possible english cv and vc syllables . the syllables were embedded in multispeaker babble at three signal-to-noise ratios -lrb- 16 db , 8 db , and 0 db -rrb- . effects of signal-to-noise ratio on vowel and consonant identification are discussed as a function of syllable position and of relationship to the native phoneme inventory . comparison of the results with previously reported data from native listeners reveals that noise affected the responding of native and non-native listeners similarly .
from an audio perspective , the present state of teleconferencing technology leaves something to be desired ; speaker overlap is one of the causes of this inadequate performance . to that end , this paper presents a frequency-domain implementation of convolutive bss specifically designed for the nature of the teleconferencing environment . in addition to presenting a novel depermutation scheme , this paper presents a least-squares post-processing scheme , which exploits segments during which only a subset of all speakers are active . experiments with simulated and real data demonstrate the ability of the proposed least-squares post-processing scheme to provide sirs at or near that of the adaptive noise cancellation solution which is obtained under idealistic assumptions that the adaptive noise cancellation solution are adapted with one source being on at a time .
the paper presents a new application of automatic speech processing in the ambient assisted living area , developed in the course of a three year research project . recording and automatic processing of spoken conversations plays a major role in this solution enabling effective search in a personal audio archive and fast browsing of conversations . processing of elderly conversational speech recorded by a distant pda microphone poses a great challenge . the speech processing flow includes transcription , speaker tracking and combined indexing and search of spoken terms and participating speakers identity extracted from the audio . we present the entire application and individual speech processing components as well as evaluation results of the individual components and of the end-to-end spoken information retrieval solution .
our study deals with a silent speech interface based on mapping surface electromyographic signals to speech waveforms . electromyographic signals recorded from the facial muscles capture the activity of the human articulatory apparatus and therefore allow to retrace speech , even when no audible signal is produced . the mapping of emg signals to speech is done via a gaussian mixture model - based conversion technique . in this paper , we follow the lead of emg-based speech-to-text systems and apply two major recent technological advances to our system , namely , we consider emg-based speech-to-text systems , which are robust against electrode repositioning , and we show that mapping the emg signal to whispered speech creates a better speech signal than a mapping to normally spoken speech . we objectively evaluate the performance of our systems using a spectral distortion measure .
the recent proliferation of large multimedia collections has gathered immense attention from the speech research community , because speech recognition enables the transcription and indexing of such large multimedia collections . topicality information can be used to improve transcription quality and enable content navigation . in this paper , we give a novel quality measure for topic segmen-tation algorithms that improves over previously used measures . our quality measure takes into account not only the presence or absence of topic boundaries but also the content of the text or speech segments labeled as topic-coherent . additionally , we demonstrate that topic segmentation quality of spoken language can be improved using speech recognition lattices . using lattices , improvements over the baseline one-best topic model are observed when measured with the previously existing topic segmentation quality measure , as well as the new quality measure proposed in this paper -lrb- 9.4 % and 7.0 % relative error reduction , respectively -rrb- .
this paper focuses on the novel task of automatic extraction of phrases related to causes of emotions . the analysis of emotional causes in sentences , where emotions are explicitly indicated through emotion keywords can provide the foundation for research on challenging task of recognition of implicit affect from text . we developed a corpus of emotion causes specific for 22 emotions . based on the analysis of this corpus we introduce a method for the detection of the linguistic relations between an emotion and its cause and the extraction of the phrases describing the emotion causes . the method employs syntactic and dependency parser and rules for the analysis of eight types of the emotion-cause linguistic relations . the results of evaluation showed that our method performed with high level of accuracy -lrb- 82 % -rrb- .
we introduce a new class of distinguished regions based on detecting the most salient convex local arrangements of contours in the image . the regions are used in a similar way to the local interest points extracted from gray-level images , but they capture shape rather than texture . local convexity is characterized by measuring the extent to which the detected image contours support circle or arc-like local structures at each position and scale in the image . our class of distinguished regions combines two cost functions defined on the tangential edges near the circle : a tangential-gradient energy term , and an entropy term that ensures local support from a wide range of angular positions around the circle . the detected regions are invariant to scale changes and rotations , and robust against clutter , occlusions and spurious edge detections . experimental results show very good performance for both shape matching and recognition of object categories .
in this paper , a session variability subspace projection svspbased model compensation method for speaker verification is proposed . during the training phase the session variability is removed from speaker models by projection , while during the testing phase the session variability in a test utterance is used to compensate speaker models . finally , the compensated speaker models and ubm are used to recognize the identity of the test utterance . compared with the conventional gmm-ubm system , the relative equal error rate reduction of session variability subspace projection svspbased model compensation method is 16.2 % on the nist 2006 single-side one conversation training , single-side one conversation test .
the status quo approach to training object detectors requires expensive bounding box annotations . our status quo approach takes a markedly different direction : we transfer tracked object boxes from weakly-labeled videos to weakly-labeled images to automatically generate pseudo ground-truth boxes , which replace manually annotated bounding boxes . we first mine discriminative regions in the weakly-labeled image collection that frequently/rarely appear in the posi-tive/negative images . we then match those regions to videos and retrieve the corresponding tracked object boxes . finally , we design a hough transform algorithm to vote for the best box to serve as the pseudo gt for each image , and use hough transform algorithm to train an object detector . together , discriminative regions lead to state-of-the-art weakly-supervised detection results on the pascal 2007 and 2010 datasets .
in this paper , we investigate the performance of acoustic equalization in reverberant environments . we first highlight an efficient general acoustic equalization of a sound field using spherical harmonics . we then use this acoustic equalization to develop a concise closed-form expression for robustness of equalization to sensor movement . this concise closed-form expression is used -lrb- i -rrb- to characterize equalization performance for a general class of non-isotropic sound fields and -lrb- ii -rrb- to quantify the improvements to equalizer robustness that can be obtained by using a directional microphone . the concise closed-form expression used here does not use any of the assumptions of statistical acoustics , but instead exploits the inherent properties of a sound field as described by the wave equation .
robust m-periodogram is defined for the analysis of signals with heavy-tailed distribution noise . in the form of a robust spectrogram -lrb- rspec -rrb- rspec based instantaneous frequency estimator can be used for the analysis of nonstationary signals . in this paper a rspec based instantaneous frequency estimator , with a time-varying window length , is presented . the optimal choice of the window length can resolve the bias-variance trade-off in the rspec based if estimation . however , rspec based instantaneous frequency estimator depends on the unknown nonlinearity of the rspec based instantaneous frequency estimator . the rspec based instantaneous frequency estimator used in this paper is able to provide the accuracy close to the one that could be achieved if the rspec based instantaneous frequency estimator , to be estimated , were known in advance . simulations show good accuracy ability of the adaptive algorithm and good robustness property with respect to rare high magnitude noise values .
we present a robust approach to modeling voiced speech using a family of minimum variance distortionless response -lrb- mvdr -rrb- spectral estimates . the method exploits the fact that for a ſxed model order , for a sinusoidal signal in noise , the mvdr estimate at the sinusoidal frequency is approximately related to the sinusoidal and noise power in a simple linear manner with the coefſcients being dependent on the model order . modeling voiced speech as a sum of harmonic signals , we then use the aforementioned relationship along with a least squares approach to combine a family of mvdr estimates -lrb- mvdr estimates of different orders -rrb- and develop a robust approach for modeling voiced speech . experimental results of spectral estimation of sinusoids , synthetic vowels , and actual speech signals at mvdr estimates -lrb- mvdr estimates of 0 db and 5 db using this approach indicate an increased resolution in the estimated mvdr spectra . the mvdr estimates -lrb- mvdr estimates computed from the mvdr estimate using this approach are also used for speaker identiſcation experiments on the timit database at various mvdr estimates -lrb- mvdr estimates . the results indicate a reasonable improvement in recognition performance when compared to the mvdr estimates -lrb- mvdr estimates and the ſxed order mvdr-mfcc .
the recently introduced m-vector approach uses maximum likelihood linear regression super-vectors for speaker verification , where maximum likelihood linear regression super-vectors are estimated with respect to a universal background model without any transcription of speech segments and speaker m-vectors are obtained by uniform segmentation of their maximum likelihood linear regression super-vectors . hence , this m-vector approach does not exploit the phonetic content of the speech segments . in this paper , we propose the integration of an automatic speech recognition -lrb- asr -rrb- based multi-class mllr transformation into the m-vector approach . we consider two variants , with maximum likelihood linear regression super-vectors computed either on the 1-best -lrb- hypothesis -rrb- or on the lattice word transcriptions . the former case is able to account for the risk of asr transcription errors . we show that the proposed m-vector approach outperform the conventional m-vector approach over various tasks of the nist sre 2008 core condition .
in this paper , we develop a new method for weighted least squares 2d linear-phase fir filter design . it poses the problem of filter design as the problem of projecting the desired frequency response onto the subspace spanned by an appropriate orthonormal basis . we show how to compute the orthonormal basis efficiently in the cases of quadrantally-symmetric filter design and centro-symmetric filter design . the design examples show that the proposed method is faster than a conventional weighted least squares filter design method . also , the amount of storage required to compute the filter coefficients is greatly reduced .
this paper presents our entry to a speech-in-noise intelligibility enhancement evaluation : the hurricane challenge . the system consists of a text-to-speech voice manipulated through a combination of enhancement strategies , each of which is known to be individually successful : a perceptually-motivated spectral shaper based on the glimpse proportion measure , dynamic range compression , and adaptation to lombard excitation and duration patterns . we achieved substantial intelligibility improvements relative to unmodified synthetic speech : 4.9 db in competing speaker and 4.1 db in speech-shaped noise . an analysis conducted across this and other two similar evaluations shows that the spectral shaper and the compressor -lrb- both of which are loudness boosters -rrb- contribute most under higher snr conditions , particularly for speech-shaped noise . duration and excitation lombard-adapted changes are more beneficial in lower snr conditions , and for competing speaker noise .
segmental dynamic time warping -lrb- dtw -rrb- has been demonstrated to be a useful technique for finding acoustic similarity scores between segments of two speech utterances . due to its high computational requirements , it had to be computed in an offline manner , limiting the applications of the technique . in this paper , we present results of parallelization of this task by distributing the workload in either a static or dynamic way on an 8-processor cluster and discuss the trade-offs among different distribution schemes . we show that online unsupervised pattern discovery using segmental dtw is plausible with as low as 8 processors . this brings the task within reach of today 's general purpose multi-core servers . we also show results on a 32-processor system , and discuss factors affecting scalability of our methods .
digital images nowadays show large appearance vari-abilities on picture styles , in terms of color tone , contrast , vignetting , and etc. . these ` picture styles ' are directly related to the scene radiance , image pipeline of the camera , and post processing functions -lrb- e.g. , photography effect filters -rrb- . due to the complexity and nonlinearity of these factors , popular gradient-based image descriptors generally are not invariant to different picture styles , which could degrade the performance for object recognition . given that images shared online or created by individual users are taken with a wide range of devices and may be processed by various post processing functions , to find a robust object recognition system is useful and challenging . in this paper , we investigate the influence of picture styles on object recognition by making a connection between image de-scriptors and a pixel mapping function g , and accordingly propose an adaptive approach based on a g-incorporated kernel descriptor and multiple kernel learning , without estimating or specifying the image styles used in training and testing . we conduct experiments on the domain adaptation data set , the oxford flower data set , and several variants of the flower data set by introducing popular photography effects through post-processing . the results demonstrate that the proposed adaptive approach consistently yields recognition improvements over standard descriptors in all studied cases .
this paper describes a new dialogue control method that utilizes new recognition processes called '' presupposition-type recognition '' and '' pretense-type recognition '' that we propose based on human dialogue analysis . this dialogue control method provides users with stress-free voice input through real-time responses , comprising a naturally controlled dialogue to obtain information in order to winnow candidates comprehensively .
we present an algorithm for reverberant speech enhancement using one microphone . we first propose a novel pitch-based reverberation measure for estimating reverberation time based on the distribution of relative time lags . this measure of pitch strength correlates with reverberation and decreases systematically as detrimental effects of reverberation on harmonic structure increase . then a reverberant speech enhancement method is developed to estimate and subtract later echo components . the results show that our approach appreciably reduces reverberation effects .
with improved recognition accuracies for lvcsr tasks , it has become possible to search large collections of spontaneous speech for a variety of information . the malach corpus of holocaust testimonials is one such collection , in which we are interested in automatically transcribing and retrieving portions that are relevant to named entities such as people , places , and organizations . since the testimonials were gathered from thousands of people in countries throughout europe , an extremely large number of potential named entities are possible , and this causes a well-known dilemma : increasing the size of the vocabulary allows for more of these words to be recognized , but also increases confusability , and can harm recognition performance . however , the malach corpus of holocaust testimonials , like many other collections , includes side information or malach corpus of holocaust testimonials that can be exploited to provide prior information on exactly which named entities are likely to appear . this paper proposes a method that capitalizes on this prior information to reduce named-entity recognition errors by over 50 % relative , and simultaneously decrease the overall word error rate by 7 % relative . the malach corpus of holocaust testimonials we use derives from a pre-interview questionaire that includes the names of friends , relatives , places visited , membership of organizations , synonyms of place names , and similar information . by augmenting the lexicon and language model with this information on a speaker-by-speaker basis , we are able to exploit the textual information that is already available in the corpus to facilitate much improved speech recognition .
we describe a unified formulation and algorithm to find an extremely sparse representation for calcium image sequences in terms of cell locations , cell shapes , spike timings and impulse responses . solution of a single optimization problem yields cell segmentations and activity estimates that are on par with the state of the art , without the need for heuristic pre-or postprocessing . experiments on real and synthetic data demonstrate the viability of the proposed method .
a method of localising objects in images is proposed . possible conngurations are evaluated using the contour discriminant , a likelihood ratio which is derived from a probabilistic model of the feature detection process . we treat each step in this process probabilistically , including the occurrence of clutter features , and derive the observation densities for both correct \ target '' con-gurations and incorrect \ clutter '' conngurations . the contour discriminant distinguishes target objects from the background even in heavy clutter , making only the most general assumptions about the form that clutter might take . the method generates samples stochasti-cally to avoid the cost of processing an entire image , and promises to be particularly suited to the task of initialising contour trackers based on sampling methods .
most hmm-based speech recognition systems use gaussian mixtures as observation probability density functions . an important goal in all such hmm-based speech recognition systems is to improve parsimony . one method is to adjust the type of covariance matrices used . in this work , fac-tored sparse inverse covariance matrices are introduced . based on í 1/4 í factorization , the inverse covariance matrix can be represented using linear regressive coefficients which 1 -rrb- correspond to sparse patterns in the inverse covariance matrix -lrb- and therefore represent conditional independence properties of the gaussian -rrb- , and 2 -rrb- , result in a method of partial tying of the covariance matrices without requiring non-linear em update equations . results show that the performance of full-covariance gaussians can be matched by factored sparse inverse covariance gaussians having significantly fewer parameters .
this paper describes ongoing research on a japanese-to-english speech-to-speech translation system for '' controlled monologue '' , such as tv news and commentary programs in which the speaking styles are controlled as a monologue . we have adopted the data-driven approach since the tv programs in question cover a wide range of topics , and because it seems much too labor intensive to handcraft translation rules . the data-driven approach therefore requires a gigantic parallel corpus for the target domain , although the absolute size needed is not so easy to obtain . there are also difficulties inherent in monologue such as the need to handle long sentences , averaging over 25 words , and the realization of simultaneity . these problems are presented in the light of our available corpora and we go on to present the kinds of problems we have to solve . finally , we present our prospective system architecture and introduce the present status of the work .
in this paper , we propose a novel radius control strategy for sphere decoding referred to as inter search radius control that provides further improvement of the computational complexity with minimal extra cost and negligible performance penalty . the proposed radius control strategy focuses on the sphere radius control strategy when a candidate lattice point is found . for this purpose , the dynamic radius update strategy as well as the lattice independent radius selection scheme are jointly exploited . from simulations in multiple-input and multiple-output channels , it is shown that the proposed radius control strategy provides a substantial improvement in complexity with near-ml performance .
we present a variational bayesian framework for performing inference , density estimation and model selection in a special class of graphical models -- hidden markov random fields . hidden markov random fields are particularly well suited to image modelling and in this paper , we apply hidden markov random fields to the problem of image segmentation . unfortunately , hidden markov random fields are notoriously hard to train and use because the exact inference problems they create are intractable . our main contribution is to introduce an efficient variational approach for performing approximate inference of the variational bayesian framework of hidden markov random fields , which we can then apply to the density estimation and model selection problems that arise when learning image models from data . with this variational approach , we can conveniently tackle the problem of image segmentation . we present experimental results which show that our variational bayesian framework outperforms recent hmrf-based segmentation methods on real world images .
recently a large amount of research has been devoted to automatic activity analysis . typically , activities have been defined by their motion characteristics and represented by trajectories . these trajectories are collected and clustered to determine typical behaviors . this paper evaluates different similarity measures and clustering methodologies to catalog their strengths and weaknesses when utilized for the trajectory learning problem . the clustering performance is measured by evaluating the correct clustering rate on different datasets with varying characteristics .
frequency warping using allpass structures or laguerre filters has found increasingly applications in audio signal processing due to good match with the auditory frequency resolution . kautz filters are an extension where the frequency warping and related resolution can have more freedom . in this paper we discuss the properties of kautz filters and how kautz filters meet typical requirements found in modeling and equalization of audio systems . case studies include transfer function modeling of the guitar body and loudspeaker response equalization .
in this paper we propose a class of efficient generalized method-of-moments algorithms for computing parameters of the plackett-luce model , where the data consists of full rankings over alternatives . our generalized method-of-moments algorithms is based on breaking the full rankings into pairwise comparisons , and then computing parameters that satisfy a set of generalized moment conditions . we identify conditions for the output of generalized method-of-moments algorithms to be unique , and identify a general class of consistent and inconsistent breakings . we then show by theory and experiments that our generalized method-of-moments algorithms run significantly faster than the classical minorize-maximization algorithm , while achieving competitive statistical efficiency .
word sense disambiguation -lrb- word sense disambiguation systems -rrb- systems based on supervised learning achieved the best performance in sense-val and semeval workshops . however , there are few publicly available supervised english all-words wsd system . this limits the use of word sense disambiguation systems in other applications , especially for researchers whose research interests are not in word sense disambiguation systems . in this paper , we present ims , a supervised english all-words wsd system . the flexible framework of ims allows users to integrate different preprocessing tools , additional features , and different classifiers . by default , we use linear support vector machines as the classifier with multiple knowledge-based features . in our implementation , ims achieves state-of-the-art results on several senseval and semeval tasks .
harmonet , a harmonet employing connectionist networks for music processing , is presented . after being trained on some dozen bach chorales using error backpropagation , the harmonet is capable of producing four-part chorales in the style of j . s.bach , given a one-part melody . our harmonet solves a musical real-world problem on a performance level appropriate for musical practice . harmonet 's power is based on -lrb- a -rrb- a new coding scheme capturing musically relevant information and -lrb- b -rrb- the integration of backpropagation and symbolic algorithms in a hierarchical system , combining the advantages of both .
conventional video summarization methods j2 -rrb- cus predominantly on summarizing videos along the time axis , such as building a movie trailer : the resulting video trailer tends to retain much empty spuce in the background of the video ji-ames while discarding much informulive video content due lo size limit . in this pupes we propose a novel space-time video summarization method which we call space-time video montage . the space-time video summarization method simultaneously analyzes both the spatial and temporal injbrmation distribution in a video sequence , and exlructs the visually informative space-time portions of the input videos . the informative video porlions are represented in volumetric la.yers . the layers are then puckrd together in a smull ouzput video volume such tlzar the total amount of visual information in the video volume is maximized . to achieve the packing process , we develop a new space-time video summarization method based upon the3rst-jt und graph cut optimization techniques . since our space-time video summarization method is uble to cul qfr spatially und temporally less informative portions , it is uble to generute much more compuct yet highly informative output videos . the effecliveness -lrb- $ our space-time video summarization method is validated by extensive experiments over a wide variety c ~ videos .
this paper presents a new unsupervised algorithm -lrb- wordends -rrb- for inferring word boundaries from transcribed adult conversations . phone ngrams before and after observed pauses are used to bootstrap a simple dis-criminative model of boundary marking . this fast algorithm delivers high performance even on morphologically complex words in english and arabic , and promising results on accurate phonetic transcriptions with extensive pronunciation variation . expanding training data beyond the traditional miniature datasets pushes performance numbers well above those previously reported . this suggests that wordends is a viable model of child language acquisition and might be useful in speech understanding .
machine learning contains many computational bottlenecks in the form of nested summations over datasets . computation of these machine learning is typically o -lrb- n 2 -rrb- or higher , which severely limits application to large datasets . we present a multi-stage stratified monte carlo method for approximating such machine learning with probabilistic relative error control . the essential idea is fast approximation by sampling in trees . this multi-stage stratified monte carlo method differs from many previous scalability techniques -lrb- such as multi-tree methods -rrb- in that its error is stochastic , but we derive conditions for error control and demonstrate that they work . further , we give a theoretical sample complexity for the multi-stage stratified monte carlo method that is independent of dataset size , and show that this appears to hold in experiments , where speedups reach as high as 10 14 , many orders of magnitude beyond the previous state of the art .
large-scale 1-regularized loss minimization problems arise in high-dimensional applications such as compressed sensing and high-dimensional supervised learning , including classification and regression problems . high-performance algorithms and implementations are critical to efficiently solving these problems . building upon previous work on coordinate descent algorithms for 1-regularized problems , we introduce a novel family of algorithms called block-greedy coordinate descent that includes , as special cases , several existing algorithms such as scd , greedy cd , shotgun , and thread-greedy . we give a unified convergence analysis for the family of block-greedy algorithms . the analysis suggests that block-greedy coordinate descent can better exploit parallelism if features are clustered so that the maximum inner product between features in different blocks is small . our theoretical convergence analysis is supported with experimental results using data from diverse real-world applications . we hope that algorithmic approaches and convergence analysis we provide will not only advance the field , but will also encourage researchers to systematically explore the design space of algorithms for solving large-scale 1-regularization problems .
a bayesian-kullback learning scheme , called ying-yang machine , is proposed based on the two complement but equivalent bayesian representations for joint density and their kullback divergence . not only the bayesian-kullback learning scheme unifies existing major supervised and unsu-pervised learnings , including the classical maximum likelihood or least square learning , the maximum information preservation , the em & em algorithm and information geometry , the recent popular helmholtz machine , as well as other learning methods with new variants and new results ; but also the bayesian-kullback learning scheme provides a number of new learning methods .
person re-identification has been widely studied due to its importance in surveillance and forensics applications . in practice , gallery images are high-resolution -lrb- hr -rrb- while probe images are usually low-resolution in the identification scenarios with large variation of illumination , weather or quality of cameras . person re-identification in this kind of scenarios , which we call super-resolution person re-identification , has not been well studied . in this paper , we propose a semi-coupled low-rank discriminant dictionary learning -lrb- sld2l -rrb- approach for sr person re-identification task . with the hr and lr dictionary pair and mapping matrices learned from the features of hr and lr training images , sld2l can convert the features of lr probe images into hr features . to ensure that the converted features have favorable discriminative capability and the learned dictionaries can well characterize intrinsic feature spaces of hr and lr images , we design a discriminant term and a low-rank regularization term for sld2l . moreover , considering that low resolution results in different degrees of loss for different types of visual appearance features , we propose a multi-view sld2l approach , which can learn the type-specific dictionary pair and mappings for each type of feature . experimental results on multiple publicly available datasets demonstrate the effectiveness of our proposed approaches for the sr person re-identification task .
| multiple source signals impinging on an antenna array can be separated by time-frequency synthesis techniques . averaging of the time-frequency distributions of the data across the array permits the spatial signatures of sources to play a fundamental role in improving the synthesis performance . array a veraging introduces a weighing function in the time-frequency domain that decreases the noise levels , reduces the interactions of the source signals , and mitigates the crossterms . this is achieved independent of the temporal characteristics of the source signals and without causing any smearing of the signal terms . the weighing function may take non-integer values , which are determined by the communication channel , the source positions and their angular separations . unlike the recently devised blind source separation methods using spatial time-frequency distributions , the proposed method does not require whitening or retrieval of the source directional matrix . the paper evaluates the proposed method in terms of performance and computations relative t o the existing source separation techniques based on quadratic t-f distributions .
we provide a new analysis of an efficient margin-based algorithm for selective sampling in classification problems . using the so-called tsybakov low noise condition to parametrize the instance distribution , we show bounds on the convergence rate to the bayes risk of both the fully supervised and the selective sampling versions of the basic algorithm . our analysis reveals that , excluding logarithmic factors , the average risk of the selective sampler converges to the bayes risk at rate n − -lrb- 1 + α -rrb- -lrb- 2 + α -rrb- / 2 -lrb- 3 + α -rrb- where n denotes the number of queried labels , and α > 0 is the exponent in the low noise condition . for all α > √ 3 − 1 ≈ 0.73 this convergence rate is asymptotically faster than the rate n − -lrb- 1 + α -rrb- / -lrb- 2 + α -rrb- achieved by the fully supervised version of the same classifier , which queries all labels , and for α → ∞ the two rates exhibit an exponential gap . experiments on textual data reveal that simple variants of the proposed selective sampler perform much better than popular and similarly efficient competitors .
we enrich a curated resource of common-sense knowledge by formulating the problem as one of knowledge base completion . most work in knowledge base completion focuses on knowledge bases like freebase that relate entities drawn from a fixed set . however , the tuples in conceptnet -lrb- speer and havasi , 2012 -rrb- define relations between an unbounded set of phrases . we develop neural network models for scoring tuples on arbitrary phrases and evaluate neural network models by their ability to distinguish true held-out tuples from false ones . we find strong performance from a bilinear model using a simple additive architecture to bilinear model phrases . we manually evaluate our trained bilinear model 's ability to assign quality scores to novel tuples , finding that bilinear model can propose tu-ples at the same quality level as medium-confidence tuples from conceptnet .
we investigate whether it is possible to improve the performance of automated facial forensic sketch matching by learning from examples of facial forgetting over time . forensic facial sketch recognition is a key capability for law enforcement , but remains an unsolved problem . it is extremely challenging because there are three distinct contributors to the domain gap between forensic sketches and photos : the well-studied sketch-photo modality gap , and the less studied gaps due to -lrb- i -rrb- the forgetting process of the eye-witness and -lrb- ii -rrb- their inability to elucidate their memory . in this paper , we address the memory problem head on by introducing a database of 400 forensic sketches created at different time-delays . based on this database we build a model to reverse the forgetting process . surprisingly , we show that it is possible to systematically '' un-forget '' facial details . moreover , it is possible to apply this model to dramatically improve forensic sketch recognition in practice : we achieve the state of the art results when matching 195 benchmark forensic sketches against corresponding photos and a 10,030 mugshot database .
we present a novel algorithm for simultaneous color and depth inpainting . the algorithm takes stereo images and estimated disparity maps as input and fills in missing color and depth information introduced by occlusions or object removal . we first complete the disparities for the occlusion regions using a segmentation-based approach . the completed disparities can be used to facilitate the user in labeling objects to be removed . since part of the removed regions in one image is visible in the other , we mutually complete the two images through 3d warping . finally , we complete the remaining unknown regions using a depth-assisted texture synthesis technique , which simultaneously fills in both color and depth . we demonstrate the effectiveness of the proposed algorithm on several challenging data sets .
in this paper , three different approaches to pronunciation modeling are investigated . two existing pronunciation modeling approaches , namely the pronunciation modeling and n-best rescoring approach are modified to work with little amount of non-native speech . we also propose a speaker clustering approach , which capable of grouping the speakers based on their pronunciation habits . given some speech , the speaker clustering approach can also be used for pronunciation modeling . this speaker clustering approach is called latent pronunciation analysis . the results show that conventional pronunciation modeling perform slightly better than n-best list rescoring , while the latent pronunciation analysis has shown to be beneficial for speaker clustering , and speaker clustering approach can produce nearly the same improvement as the pronunciation dictionary approach , without the need to know the origin of the speaker .
we consider the problem of image segmentation using active contours through the minimization of an energy criterion involving both region and boundary functionals . these image segmentation are derived through a shape derivative approach instead of classical calculus of variation . the image segmentation can be elegantly derived without converting the region integrals into boundary integrals . from the derivative , we deduce the evolution equation of an active contour that makes it evolve towards a minimum of the criterion . we focus more particularly on statistical features globally attached to the region and especially to the probability density functions of image features such as the color histogram of a region . a theoretical framework is set for the minimization of the distance between two histograms for matching or tracking purposes . an application of this theoretical framework to the segmentation of color histograms in video sequences is then proposed . we briefly describe our numerical scheme and show some experimental results .
we investigate the computational complexity of testing dominance and consistency in cp-nets . up until now , the complexity of dominance has been determined only for restricted classes in which the dependency graph of the cp-nets is acyclic . however , there are preferences of interest that define cyclic dependency graphs ; cyclic dependency graphs are modeled with general cp-nets . we show here that both dominance and consistency testing for general cp-nets are pspace-complete . the reductions used in the proofs are from strips planning , and thus establish strong connections between both areas .
this paper addresses the problem of merging speech enhancement and coding in the context of an auditory modeling . the noisy signal is rst processed by a fast wavelet packet transform algorithm to obtain an auditory spectrum , from which a rough masking model is estimated . then , this model is used to rene a subtractive-type enhancement algorithm . the enhanced speech coecients are then encoded in the same time-frequency transform domain using masking threshold constraints for quantization noise . the advantage of the proposed method is that both enhancement and coding are performed with the transform coecients , without making use of the additional fft processing .
symmetry reduction has significantly contributed to the success of classical planning as heuristic search . however , it is an open question if symmetry reduction techniques can be lifted to fully observable nondeterministic planning . we generalize the concepts of structural symmetries and symmetry reduction to fond planning and specifically to the lao ⇤ algorithm . our base implementation of lao ⇤ algorithm in the fast downward planner is competitive with the lao ⇤ algorithm - based fond planner mynd . our experiments further show that symmetry reduction can yield strong performance gains compared to our base implementation of lao ⇤ algorithm .
