this paper describes an experiment comparing the effect of two different approaches to information presentation on item recall . the results show that using discourse cues facilitates recalling the presented information .
belief revision is a ubiquitous process underlying many forms of intelligent behaviour . the agm paradigm is a powerful framework for modeling and implementing belief revision systems based on the principle of minimal change ; agm paradigm provides a rich and rigorous foundation for computer-based belief revision architectures . maxi-adjustment is a belief revision strategy for theory bases that can be implemented using a standard theorem prover , and one that has been used successfully for several applications . in this paper we provide an anytime decision procedure for maxi-adjustments , and study its complexity . furthermore , we outline a set of guidelines that serve as a protomethodology for building belief revision systems employing a maxi-adjustment . the anytime decision procedure is under development in the belief revision module of the cin project .
in this paper , we propose a novel general framework for ten-sor based null space affine invariants , namely , tensor null space invariants with a linear classifier for high order data classification and retrieval . we first derive tensor null space invariants , which is perfectly invariant to multidimensional affine transformations due to camera motions for multiple motion trajectories in consecutive motion events . we subsequently propose an efficient classification and retrieval system relying on tensor null space invariants for archiv-ing and searching motion events consisting of multiple motion trajectories . the simulation results demonstrate superior performance of the proposed classification and retrieval system .
in an extended image sequence of an outdoor scene , one observes changes in color induced by variations in the spectral composition of daylight . this paper proposes a model for these temporal color changes and explores its use for the analysis of outdoor scenes from time-lapse video data . we show that the time-varying changes in direct sunlight and ambient skylight can be recovered with this model , and that an image sequence can be decomposed into two corresponding components . the decomposition provides access to both radiometric and geometric information about a scene , and we demonstrate how this can be exploited for a variety of visual tasks , including color-constancy , background subtraction , shadow detection , scene reconstruction , and camera geo-location .
we present a unified model for face detection , pose estimation , and landmark estimation in real-world , cluttered images . our unified model is based on a mixtures of trees with a shared pool of parts ; we unified model every facial landmark as a part and use global mixtures to capture topological changes due to viewpoint . we show that tree-structured models are surprisingly effective at capturing global elastic deformation , while being easy to optimize unlike dense graph structures . we present extensive results on standard face benchmarks , as well as a new '' in the wild '' annotated dataset , that suggests our unified model advances the state-of-the-art , sometimes considerably , for all three tasks . though our unified model is modestly trained with hundreds of faces , unified model compares favorably to commercial systems trained with billions of examples -lrb- such as google picasa and face.com -rrb- .
an associative memory is a structure learned from a dataset m of vectors -lrb- signals -rrb- in a way such that , given a noisy version of one of the vectors as input , the nearest valid vector from m -lrb- nearest neighbor -rrb- is provided as output , preferably via a fast iterative algorithm . traditionally , binary -lrb- or q-ary -rrb- hopfield neural networks are used to model the above structure . in this paper , for the first time , we propose a model of associative memory based on sparse recovery of signals . our basic premise is simple . for a dataset , we learn a set of linear constraints that every vector in the dataset must satisfy . provided these linear constraints possess some special properties , it is possible to cast the task of finding nearest neighbor as a sparse recovery problem . assuming generic random models for the dataset , we show that it is possible to store super-polynomial or exponential number of n-length vectors in a neural network of size o -lrb- n -rrb- . furthermore , given a noisy version of one of the stored vectors corrupted in near-linear number of coordinates , the vector can be correctly recalled using a neurally feasible algorithm .
efficient two-step algorithms are described for optimizing the stopband response of the prototype filter for cosine-modulated and modified dft filter banks either in the minimax or in the least-mean-square sense subject to the maximum allowable aliasing and amplitude errors . the first step involves finding a good start-up solution using a simple technique . this start-up solution is improved in the second step by using nonlinear optimization . several examples are included illustrating the flexibility of the proposed start-up solution for making compromises between the required filter lengths and the aliasing and amplitude errors . these examples show that by allowing very small amplitude and aliasing errors , the stopband performance of the resulting prototype filter is significantly improved compared to the corresponding perfect-reconstruction filter bank . alternatively , the filter orders and , consequently , the overall delay can be significantly reduced to achieve practically the same performance .
we resolve a several-years old open question in visibility-based pursuit evasion : how many pursuers are needed to capture an evader in an arbitrary polygonal environment with obstacles ? the evader is assumed to be adversarial , moves with the same maximum speed as pursuers , and is '' sensed '' by a pursuer only when it lies in line-of-sight of that pursuer . the players move in discrete time steps , and the capture occurs when a pursuer reaches the position of the evader on its move . our main result is that o -lrb- √ h + log n -rrb- pursuers can always win the game with a deterministic search strategy in any polygon with n vertices and h obstacles -lrb- holes -rrb- . in order to achieve this bound , however , we argue that the environment must satisfy a minimum feature size property , which essentially requires the minimum distance between any two vertices to be of the same order as the speed of the players . without the minimum feature size assumption , we show that ω -lrb- n / log n -rrb- pursuers are needed in the worst-case even for simply-connected polygons of n vertices ! this reveals an unexpected subtlety that seems to have been overlooked in previous work claiming that o -lrb- log n -rrb- pursuers can always win in simply-connected n-gons . our lower bound also shows that capturing an evader is inherently more difficult than just '' seeing '' it because o -lrb- log n -rrb- pursuers are prov-ably sufficient for line-of-sight detection even against an arbitrarily fast evader in simple n-gons .
a new speech enhancement system , which is based on a time-frequency adaptive wavelet soft thresholding , is presented in this paper . the speech enhancement system utilises a bark-scaled wavelet packet decomposition integrated into a modified weiner filtering technique using a novel threshold estimation method based on a magnitude decision-directed approach . first , a bark-scaled wavelet packet transform is used to decompose the speech signal into critical bands . threshold estimation is then performed for each wavelet band according to an adaptive noise level-tracking algorithm . finally , the speech is estimated by incorporating the computed threshold into a wiener filtering process , using the magnitude decision-directed approach . the proposed speech enhancement technique has been tested with various stationary and non-stationary noise cases . reported results show that the speech enhancement system is capable of a high-level of noise suppression while preserving the intelligibility and naturalness of the speech .
efficient learning equilibrium -lrb- efficient learning equilibrium -rrb- is a natural solution concept for multi-agent encounters with incomplete information . it requires the learning algorithms themselves to be in equilibrium for any game selected from a set of -lrb- initially unknown -rrb- games . in an optimal efficient learning equilibrium , the learning algorithms would efficiently obtain the surplus the agents would obtain in an optimal nash equilibrium of the initially unknown game which is played . the crucial part is that in an ele deviations from the learning algorithms would become non-beneficial after polynomial time , although the game played is initially unknown . while appealing conceptually , the main challenge for establishing learning algorithms based on this concept is to isolate general classes of games where an efficient learning equilibrium exists . unfortunately , it has been shown that while an efficient learning equilibrium exists for the setting in which each agent can observe all other agents ' actions and payoffs , an efficient learning equilibrium does not exist in general when the other agents ' payoffs can not be observed . in this paper we provide the first positive results on this problem , constructively proving the existence of an optimal efficient learning equilibrium for the class of symmetric games where an agent can not observe other agents ' payoffs .
this paper applies a dynamic sinusoidal synthesis model to statistical parametric speech synthesis . for this , we utilise regularised cepstral coefficients to represent both the static amplitude and dynamic slope of selected sinusoids for statistical modelling . during synthesis , a dynamic sinusoidal synthesis model is used to reconstruct speech . a preference test is conducted to compare the selection of different sinusoids for cepstral representation . our results show that when integrated with statistical parametric speech synthesis , a relatively small number of sinusoids selected according to a perceptual criterion can produce quality comparable to using all harmonics . a mean opinion score test shows that our proposed dynamic sinusoidal synthesis model is preferred to one using mel-cepstra from pitch synchronous spectral analysis .
in a lifelong learning framework , an lifelong learning framework acquires knowledge incrementally over consecutive learning tasks , continually building upon its experience . recent lifelong learning algorithms have achieved nearly identical performance to batch multi-task learning methods while reducing learning time by three orders of magnitude . in this paper , we further improve the scalability of lifelong learning by developing curriculum selection methods that enable an lifelong learning framework to actively select the next task to learn in order to maximize performance on future learning tasks . we demonstrate that active task selection is highly reliable and effective , allowing an lifelong learning framework to learn high performance models using up to 50 % fewer tasks than when the lifelong learning framework has no control over the task order . we also explore a variant of transfer learning in the lifelong learning setting in which the lifelong learning framework can focus knowledge acquisition toward a particular target task .
bilingual speakers are known for their ability to code-switch or mix their languages during communication . this phenomenon occurs when bilinguals substitute a word or phrase from one language with a phrase or word from another language . for code-switching speech recognition , it is essential to collect a large-scale code-switching speech database for model training . in order to ease the negative effect caused by the data sparseness problem in training code-switching speech recognizers , this study proposes a data-driven approach to phone set construction by integrating acoustic features and cross-lingual context-sensitive articulatory features into distance measure between phone units . kl-divergence and a hierarchical phone unit clustering algorithm are used in this study to cluster similar phone units to reduce the need of the training data for model construction . the experimental results show that the proposed data-driven approach outperforms other traditional phone set construction methods .
this paper introduces matrix filters as a tool for localiza-tion and detection problems in passive sonar . the outputs of an array of sensors , at some given frequency , can be represented by a vector of complex numbers . a linear filtering operation on the sensor outputs can be expressed as the multiplication of a matrix -lrb- called a matrix filters -rrb- times this vector . the purpose of a matrix filters is to attenuate unwanted components in the measured sensor data while passing desired components with minimal distortion . matrix filters are designed by defining an appropriate pass band and stop band and solving a convex optimization problem . this paper formulates the design of matrix filters for passive sonar and gives two examples .
communication among participants -lrb- agents , robots -rrb- is central to an appearance of collective ai . in this work we deal with the development of local communication mechanisms for real microro-botic swarms . we demonstrate that despite of very limited capabilities of the microrobot , the specific construction of communication hardware and software allows very extended collective capabilities of the whole swarm . we propose local communication mechanisms providing information content and context for collective navigation , coordination and spatial perception in a group of microrobots .
1 this paper presents an analysis of lombard speech produced under different types and levels of noise . the speech used for the analysis forms a part of the ut-scope database and consists of sentences from the well-known timit corpus , spoken in the presence of highway , large crowd and pink noise . differences are shown to exist in the speech characteristics under these varying noise types . the deterioration of the eer of an inset speaker identification system trained on neutral and tested with lombard speech is also illustrated . a clear demarcation between the effect of noise and lombard effect on noise is also given by testing with noisy lombard speech . the effect of test-token duration on system performance under the lombard condition is addressed . it is seen that test duration has no effect on the eer under lombard effect . the average eer for 3s test duration is 14 .
confusion matrices have been widely used to increase the accuracy of speech recognisers , but usually a mean confusion matrix , averaged over many speakers , is used . however , analysis shows that confusion matrices for individual speakers vary considerably , and so there is benefit in obtaining estimates of confusion matrices for individual speakers . unfortunately , there is rarely enough data to make reliable estimates . we present a technique for estimating the elements of a speaker 's confusion matrix given only sparse data from the speaker . it utilizes non-negative matrix factorisation to find structure within confusion matrices , and this structure is exploited to make improved estimates . results show that under certain conditions , this technique can give estimates that are as good as those obtained with twice the number of utterances available from the speaker .
we address the problem of automatically learning the main steps to complete a certain task , such as changing a car tire , from a set of narrated instruction videos . the contributions of this paper are threefold . first , we develop a joint model for video and natural language narration that takes advantage of the complementary nature of the two signals . second , we collect an annotated dataset of 57 internet instruction videos containing more than 350,000 frames for two tasks -lrb- changing car tire and cardiopulmonary resuscitation -rrb- . third , we experimentally demonstrate that the proposed joint model automatically discovers , in an unsupervised manner , the main steps to achieve each task and locate them within the input videos . the results further show that the proposed joint model outperforms single-modality baselines , demonstrating the benefits of joint modeling video and text .
discriminative approaches to human pose inference involve mapping visual observations to articulated body configurations . current probabilistic approaches to learn this mapping visual observations have been limited in their ability to handle domains with a large number of activities that require very large training sets . we propose an online probabilistic regression scheme for efficient inference of complex , high-dimensional , and multimodal mappings . our online probabilistic regression scheme is based on a local mixture of gaussian processes , where locality is defined based on both appearance and pose , and where the mapping hyperparameters can vary across local neighborhoods to better adapt to specific regions in the pose space . the mapping hyperparameters are defined online in very small neighborhoods , so learning and inference is extremely efficient . when the mapping visual observations is one-to-one , we derive a bound on the approximation error of local regression -lrb- vs. global regression -rrb- for monotonically decreasing co-variance functions . our online probabilistic regression scheme can determine when training examples are redundant given the rest of the database , and use this criteria for pruning . we report results on synthetic -lrb- poser -rrb- and real pose databases , obtaining fast and accurate pose estimates using training set sizes up to 10 5 .
in this paper we propose a robust iterative hard thresolding algorithm for reconstructing sparse signals in the presence of impulsive noise . to address this problem , we use a lorentzian cost function instead of the í µí ° ¿ 2 cost function employed by the traditional iht algorithm . the derived robust iterative hard thresolding algorithm is comparable in computational load to the least squares based iht . analysis of the proposed robust iterative hard thresolding algorithm demonstrates its robustness under heavy-tailed models . simulations show that the proposed robust iterative hard thresolding algorithm significantly outperform commonly employed sparse reconstruction techniques in impulsive environments , while providing comparable reconstruction quality in less demanding , light-tailed environments .
lexical resources such as wordnet and verbnet are widely used in a multitude of nlp tasks , as are annotated corpora such as treebanks . often , the resources are used as-is , without question or examination . this practice risks missing significant performance gains and even entire techniques . this paper addresses the importance of resource quality through the lens of a challenging nlp task : detecting selec-tional preference violations . we present david , a simple , lexical resource-based preference violation detector . with as-is lexical resources , david achieves an f 1-measure of just 28.27 % . when the resource entries and parser outputs for a small sample are corrected , however , the f 1-measure on that sample jumps from 40 % to 61.54 % , and performance on other examples rises , suggesting that the algorithm becomes practical given refined resources . more broadly , this paper shows that resource quality matters tremendously , sometimes even more than algorithmic improvements .
acoustic models -lrb- acoustic models -rrb- of an hmm-based classifier include various types of hidden variables such as gender type , speaking rate , and acoustic environment . if there exists a canonicalization process that reduces the influence of the hidden variables from the acoustic models , a robust automatic speech recognition system can be realized . in this paper , we describe the configuration of a canonicalization process targeting gender type as a hidden variable . the proposed canonicalization process is composed of multiple distinctive phonetic feature extractors corresponding to the hidden variable and a dpf selector in which the distance between input dpf and acoustic models is compared . in a dpf extraction stage , an input sequence of acoustic feature vectors is mapped onto three dpf spaces corresponding to male , female , and neutral voice by using three multilayer neural networks -lrb- mlns -rrb- . experiments are carried out by comparing -lrb- a -rrb- the combination of the canonicalized dpf and a single hmm classifier , and -lrb- b -rrb- the combination of a single acoustic feature and multiple hmm classifiers . the result shows that the proposed canonicalization process outperforms both of the conventional robust automatic speech recognition system with acoustic feature and a single hmm and the robust automatic speech recognition system with multiple hmms in spite of less memories and computation time .
recently , esprit-based parameter estimation algorithms have been developed to exploit the structure of signals from strictly second-order -lrb- so -rrb- non-circular -lrb- nc -rrb- sources . they achieve a higher estimation accuracy and can resolve up to twice as many sources . however , these nc methods assume that all the received signals are strictly non-circular . in this paper , we present the c-nc standard esprit and the c-nc unitary esprit algorithms designed for the more practical scenario of a received mixture of circular and strictly non-circular signals . assuming that the number of circular and strictly non-circular signals is known , the two proposed esprit-based parameter estimation algorithms yield closed-form estimates and c-nc unitary esprit algorithms also enables an entirely real-valued implementation . as a main result , it is shown that the estimation accuracy of the presented esprit-based parameter estimation algorithms improves with an increasing number of strictly non-circular signals among a fixed number of sources . thereby , not only the estimation accuracy of the strictly non-circular signals themselves is improved , but also the estimation accuracy of the circular signals . these results are validated by simulations .
speech separation is a challenging problem at low signal-to-noise ratios . separation can be formulated as a classification problem . in this study , we focus on the snr level of-5 db in which speech is generally dominated by background noise . in such a low snr condition , extracting robust features from a noisy mixture is crucial for successful classification . using a common neural network classifier , we systematically compare separation performance of many monaural features . in addition , we propose a new multi-resolution cochleagram called multi-resolution cochleagram , which is extracted from four cochlea-grams of different resolutions to capture both local information and spectrotemporal context . comparisons using two non-stationary noises show a range of feature robustness for speech separation with the proposed multi-resolution cochleagram performing the best . we also find that multi-resolution cochleagram , a post-processing technique previously used for robust speech recognition , improves speech separation performance by smoothing the temporal trajectories of feature dimensions .
this paper focuses on the novel task of automatic extraction of phrases related to causes of emotions . the analysis of emotional causes in sentences , where emotions are explicitly indicated through emotion keywords can provide the foundation for research on challenging task of recognition of implicit affect from text . we developed a corpus of emotion causes specific for 22 emotions . based on the analysis of this corpus we introduce a method for the detection of the linguistic relations between an emotion and its cause and the extraction of the phrases describing the emotion causes . the method employs syntactic and dependency parser and rules for the analysis of eight types of the emotion-cause linguistic relations . the results of evaluation showed that our method performed with high level of accuracy -lrb- 82 % -rrb- .
we address the problem of answering new questions in community forums , by selecting suitable answers to already asked questions . we approach the task as an answer ranking problem , adopting a pairwise neural network architecture that selects which of two competing answers is better . we focus on the utility of the three types of similarities occurring in the triangle formed by the original question , the related question , and an answer to the related comment , which we call relevance , relatedness , and appropriateness . our proposed pairwise neural network architecture models the interactions among all input components using syntactic and semantic embeddings , lexical matching , and domain-specific features . it achieves state-of-the-art results , showing that the three similarities are important and need to be mod-eled together . our experiments demonstrate that all feature types are relevant , but the most important ones are the lexical similarity features , the domain-specific features , and the syntactic and semantic embeddings .
iterative methods that take steps in approximate subgradient directions have proved to be useful for stochastic learning problems over large or streaming data sets . when the objective consists of a loss function plus a nonsmooth regularization term , whose purpose is to induce structure -lrb- for example , spar-sity -rrb- in the solution , the solution often lies on a low-dimensional manifold along which the regularizer is smooth . this paper shows that a regularized dual averaging algorithm can identify this manifold with high probability . this observation motivates an algorith-mic strategy in which , once a near-optimal manifold is identified , we switch to an regularized dual averaging algorithm that searches only in this manifold , which typically has much lower intrinsic dimension than the full space , thus converging quickly to a near-optimal point with the desired structure . computational results are presented to illustrate these claims .
we introduce parma , a parma for cross-document , semantic predicate and argument alignment . our parma combines a number of linguistic resources familiar to researchers in areas such as recognizing textual entailment and question answering , integrating linguistic resources into a simple discrimina-tive model . parma achieves state of the art results on an existing and a new dataset . we suggest that previous efforts have focussed on data that is biased and too easy , and we provide a more difficult dataset based on translation data with a low base-line which we beat by 17 % f1 .
most methods for decision-theoretic online learning are based on the hedge algorithm , which takes a parameter called the learning rate . in most previous analyses the learning rate was carefully tuned to obtain optimal worst-case performance , leading to suboptimal performance on easy instances , for example when there exists an action that is significantly better than all others . we propose a new way of setting the learning rate , which adapts to the difficulty of the decision-theoretic online learning : in the worst case our procedure still guarantees optimal performance , but on easy instances it achieves much smaller regret . in particular , our adaptive method achieves constant regret in a probabilistic setting , when there exists an action that on average obtains strictly smaller loss than all other actions . we also provide a simulation study comparing our approach to existing methods .
stress and syllabification are essential attributes for several components in text-to speech systems . stress are responsible for improving grapheme-to-phoneme conversion rules and for enhancing the synthetic intelligibility , since stress and syllable are key units in prosody prediction . this paper presents three linguistically rule-based automatic algorithms for catalan text-to-speech conversion : a word stress marker , an orthographic syllabification algorithm and a phonological syllabification algorithm . the linguistically rule-based automatic algorithms were implemented and tested . the results gave rise to the following word accuracy rates : 100 % for the stress marker algorithm , 99.7 % for the orthographic syllabification algorithm and 99.8 % for the phonological syllabification algorithm .
this paper provides an overview of the developments in auditory visual speech processing , a special interest group within eurospeech . i hope that this discussion will be informative and useful to readers in a variety of fields , including psychology , speech science , animation , psycholinguistics , human-machine interaction , hearing-impaired communication , and numerous other fields which also share in this fruitful intersection .
the map seeking circuit has been suggested to address the inverse problem of transformation discovery as found in signal processing , vision , inverse kinematics and many other natural tasks . according to this idea , a parallel search in the transformation space of a high dimensional problem can be decomposed into parts efficiently using the ordering property of superpositions . deterministic formulations of the circuit have been suggested . here , we provide a proba-bilistic interpretation of the architecture whereby the superpositions of the circuit are seen as a series of marginalisations over parameters of the transform . based on this , we interpret the weights of the map seeking circuit as importance weights . the latter suggests the incorporation of monte-carlo approaches in the map seeking circuit , providing improved resolution of parameter estimates within resource constrained implementations . as a final contribution , we model mixed serial/parallel search strategies of biological vision to reduce the problem of collusions , a common problem in the standard map seeking circuit .
in this paper , we present a novel feature normalization method in the log-scaled spectral domain for improving the noise robustness of speech recognition front-ends . in the proposed feature normalization method , a non-linear contrast stretching is added to the outputs of log mel-filterbanks to imitate the adaptation of the auditory system under adverse conditions . this is followed by a two-dimensional filter to smooth out the processing artifacts . the proposed feature normalization method perform remarkably well on censrec-2 in-car database with an average relative improvement of 29.3 % compared to baseline mfcc system . it is also confirmed that the proposed processing in log mfb domain can be integrated with conventional cepstral post-processing techniques to yield further improvements . the proposed feature normalization method is simple and requires only a small extra computation load .
this paper presents a representation theory for permutation-valued functions , which in their general form can also be called listwise ranking functions . point-wise ranking functions assign a score to each object independently , without taking into account the other objects under consideration ; whereas listwise loss functions evaluate the set of scores assigned to all objects as a whole . in many supervised learning to rank tasks , it might be of interest to use listwise ranking functions instead ; in particular , the bayes optimal ranking functions might themselves be listwise , especially if the loss function is listwise . a key caveat to using list-wise ranking functions has been the lack of an appropriate representation theory for such functions . we show that a natural symmetricity assumption that we call exchangeability allows us to explicitly characterize the set of such exchangeable listwise ranking functions . our analysis draws from the theories of tensor analysis , functional analysis and de finetti theorems . we also present experiments using a novel reranking method motivated by our representation theory .
in chinese , most of the language processing starts from word segmentation and part-of-speech tagging . these two steps tokenize the word from a sequence of characters and predict the syntactic labels for each segmented word . in this paper , we present two distinct sequential tagging models for the above two tasks . the first sequential tagging models was basically similar to previous work which made use of conditional random fields and set of predefined dictionaries to recognize word boundaries . second , we revise and modify support vector machine based chunking model to label the pos tag in the pos tagging task . our support vector machine based chunking model in the ws task achieves moderately rank among all participants , while in the pos tagging task , it reaches very competitive results .
we address the problem of detecting batches of emails that have been created according to the same template . this problem is motivated by the desire to filter spam more effectively by exploiting collective information about entire batches of jointly generated messages . the application matches the problem setting of supervised clustering , because examples of correct clusterings can be collected . known decoding procedures for supervised clustering are cubic in the number of instances . when decisions can not be reconsidered once they have been made -- owing to the streaming nature of the data -- then the decoding problem can be solved in linear time . we devise a sequential decoding procedure and derive the corresponding optimization problem of supervised clustering . we study the impact of collective attributes of email batches on the effectiveness of recognizing spam emails .
learning to hash involves learning hash functions from a set of images for embedding high-dimensional visual descriptors into a similarity-preserving low-dimensional hamming space . most of existing methods resort to a single representation of images , that is , only one type of visual descriptors is used to learn a hash function to assign binary codes to images . however , images are often described by multiple different visual descriptors -lrb- such as sift , gist , hog -rrb- , so it is desirable to incorporate these multiple representations into learning a hash function , leading to multi-view hashing . in this paper we present a sequential spectral learning approach to multi-view hashing where a hash function is sequentially determined by solving the successive maximiza-tion of local variances subject to decorrelation constraints . we compute multi-view local variances by α-averaging view-specific distance matrices such that the best averaged distance matrix is determined by minimizing its α-divergence from view-specific distance matrices . we also present a scalable implementation , exploiting a fast approximate k-nn graph construction method , in which α-averaged distances computed in small partitions determined by recursive spectral bisection are gradually merged in conquer steps until whole examples are used . numerical experiments on caltech-256 , cifar-20 , and nus-wide datasets confirm the high performance of our sequential spectral learning approach , in comparison to single-view spectral hashing as well as existing multi-view hashing methods .
the goal of speech emotion recognition is to identify the emotional or physical state of a human being from his or her voice . one of the most important things in a speech emotion recognition is to extract and select relevant speech features with which most emotions could be recognized . in this paper , we present a smoothed nonlinear energy operator - based amplitude modulation cepstral coefficients -lrb- amcc -rrb- feature for recognizing emotions from speech signals . smoothed nonlinear energy operator estimates the energy required to produce the am-fm signal , and then the estimated energy is separated into its amplitude and frequency components using an energy separation algorithm . amcc features are obtained by first decomposing a speech signal using a c-channel gammatone filterbank , computing the am power spectrum , and taking a discrete cosine transform of the root compressed am power spectrum . conventional mfcc -lrb- mel-frequency cepstral coefficients -rrb- and mel-warped dft -lrb- discrete fourier transform -rrb- spectrum based cepstral coefficients -lrb- mwdcc -rrb- features are used for comparing the recognition performances of the proposed features . emotion recognition experiments are conducted on the fau aibo spontaneous emotion corpus . it is observed from the experimental results that the amcc features provide a relative improvement of approximately 3.5 % over the baseline mfcc .
this work presents our research results on the capability of passive source localization using a towed multi-module array in underwater acoustic environments . we developed new cramer-rao lower bound results for assessing the array 's source localization capability under environmental uncertainties , such as array modules ' positions and acoustic-field 's spatial coherence . we also developed a two-stage approach and provide details for passive ranging by utilizing data either non-coherently or coherently , depending on the availability of spatial coherence in the received data from multiple modules of towed array .
many real-world decision-theoretic planning problems are naturally modeled using both continuous state and action spaces , yet little work has provided exact solutions for the case of continuous actions . in this work , we propose a symbolic dynamic programming solution to obtain the optimal closed-form value function and policy for csa-mdps with mul-tivariate continuous state and actions , discrete noise , piecewise linear dynamics , and piecewise linear -lrb- or restricted piecewise quadratic -rrb- reward . our key contribution over previous sdp work is to show how the continuous action maximization step in the dynamic programming backup can be evaluated optimally and symbolically -- a task which amounts to symbolic constrained optimization subject to unknown state parameters ; we further integrate this technique to work with an efficient and compact data structure for sdp -- the extended algebraic decision diagram -lrb- xadd -rrb- . we demonstrate empirical results on a didactic nonlinear planning example and two domains from operations research to show the first automated exact solution to these problems .
for sequential probabilistic inference in nonlinear non-gaussian systems approximate solutions must be used . we present a novel recursive bayesian estimation algorithm that combines an importance sampling based measurement update step with a bank of sigma-point kalman filters for the time-update and proposal distribution generation . the posterior state density is represented by a gaussian mixture model that is recovered from the weighted particle set of the measurement update step by means of a weighted em algorithm . this recursive bayesian estimation algorithm replaces the resampling stage needed by most particle filters and mitigates the '' sample depletion '' problem . we show that this new recursive bayesian estimation algorithm has an improved estimation performance and reduced computational complexity compared to other related algorithms .
we present a technique for coarsely extracting the regions of natural color images which contain directional detail , e.g. , edges , texture , etc. , which we then use for image database indexing . as a measure of color activity , we use a perceptually modified distance measure based on the sum-of-angles criterion . we then apply histogram thresholding techniques to separate the image into smooth color regions and busy regions where edge , texture and colour activity exists . database indices are then created from the busy regions using the direcional detail histogram technique and retrieval is performed using these .
our understanding of the input-output function of single cells has been substantially advanced by biophysically accurate multi-compartmental models . the large number of parameters needing hand tuning in these biophysically accurate multi-compartmental models has , however , somewhat hampered their applicability and inter-pretability . here we propose a simple and well-founded method for automatic estimation of many of these key parameters : 1 -rrb- the spatial distribution of channel densities on the cell 's membrane ; 2 -rrb- the spatiotemporal pattern of synaptic input ; 3 -rrb- the channels ' reversal potentials ; 4 -rrb- the in-tercompartmental conductances ; and 5 -rrb- the noise level in each compartment . we assume experimental access to : a -rrb- the spatiotemporal voltage signal in the dendrite -lrb- or some contiguous subpart thereof , e.g. via voltage sensitive imaging techniques -rrb- , b -rrb- an approximate kinetic description of the channels and synapses present in each compartment , and c -rrb- the morphology of the part of the neuron under investigation . the key observation is that , given data a -rrb- - c -rrb- , all of the parameters 1 -rrb- -4 -rrb- may be simultaneously inferred by a version of constrained linear regression ; this constrained linear regression , in turn , is efficiently solved using standard algorithms , without any '' local minima '' problems despite the large number of parameters and complex dynamics . the noise level 5 -rrb- may also be estimated by standard techniques . we demonstrate the method 's accuracy on several model datasets , and describe techniques for quantifying the uncertainty in our estimates .
we improve the automatic speech recognition of broadcast news using paradigms from web 2.0 to obtain time-and topic-relevant text data for language modeling . we elaborate an un-supervised text collection and decoding strategy that includes crawling appropriate texts from rss feeds , complementing un-supervised text collection and decoding strategy with texts from twitter , language model and vocabulary adaptation , as well as a 2-pass decoding . the word error rates of the tested french broadcast news shows from europe 1 are reduced by almost 32 % relative with an underlying language model from the globalphone project -lsb- 1 -rsb- and by almost 4 % with an underlying language model from the quaero project . the un-supervised text collection and decoding strategy that we use for the text normalization , the collection of rss feeds together with the text on the related websites , a tf-idf-based topic words extraction , as well as the opportunity for language model interpolation are available in our rapid language adaptation toolkit -lsb- 2 -rsb- -lsb- 3 -rsb- .
lexical co-occurrence is an important cue for detecting word associations . we present a theoretical framework for discovering statistically significant lexical co-occurrences from a given corpus . in contrast with the prevalent practice of giving weightage to unigram frequencies , we focus only on the documents containing both the terms -lrb- of a candidate bi-gram -rrb- . we detect biases in span distributions of associated words , while being agnostic to variations in global unigram frequencies . our framework has the fidelity to distinguish different classes of lexical co-occurrences , based on strengths of the document and corpus-level cues of co-occurrence in the data . we perform extensive experiments on benchmark data sets to study the performance of various co-occurrence measures that are currently known in literature . we find that a relatively obscure measure called ochiai , and a newly introduced measure csa capture the notion of lexical co-occurrence best , followed next by llr , dice , and ttest , while another popular measure , pmi , suprisingly , performs poorly in the context of lexical co-occurrence .
this acoustic study explored dialect effects on realization of nuclear pitch accents in three regional varieties of american english spoken in central ohio , southeastern wisconsin and western north carolina . fundamental frequency -lrb- f0 -rrb- change from vowel onset to offset in the most prominent syllable in a sentence was examined along four parameters : maximum f0 change , relative location of f0 maximum , f0 offset and f0 fall from maximum to offset . a robust finding was that the f0 contours in the southern -lrb- north carolina -rrb- variants were significantly distinct from the two midwestern varieties whose contours did not differ significantly from one another . the southern vowels had an earlier f0 rise , a greater f0 fall and a lower f0 offset than either ohio or wisconsin vowels . there was a sharper f0 drop preceding a voiceless than a voiced syllable coda . no significant dialect-related differences were found for flat f0 contours in unstressed vowels , which were also examined in the study . this study contributes the finding that dynamic variations in pitch are greater for vowels which also exhibit a greater amount of spectral dynamics . the interaction of these two sets of cues contributes to the melodic component associated with a specific regional accent .
we have recently proposed a new plda-based acoustic model based on prob-abilistic linear discriminant analysis which enjoys the flexibility of using higher dimensional acoustic features , and is more capable to capture the intra-frame feature correlations . in this paper , we investigate the use of bottleneck features obtained from a deep neural network for the plda-based acoustic model . experiments were performed on the switchboard dataset -- a large vocabulary conversational telephone speech corpus . we observe significant word error reduction by using the bottleneck features . in addition , we have also compared the plda-based acoustic model to three others using gaussian mixture models , subspace gmms and hybrid deep neural networks , and plda can achieve comparable or slightly higher recognition accuracy from our experiments .
symmetry reduction has significantly contributed to the success of classical planning as heuristic search . however , it is an open question if symmetry reduction techniques can be lifted to fully observable nondeterministic planning . we generalize the concepts of structural symmetries and symmetry reduction to fond planning and specifically to the lao ⇤ algorithm . our base implementation of lao ⇤ algorithm in the fast downward planner is competitive with the lao ⇤ algorithm - based fond planner mynd . our experiments further show that symmetry reduction can yield strong performance gains compared to our base implementation of lao ⇤ algorithm .
this paper describes the realization of a corpus-based chinese speech synthesis system , including the corpus design and unit selection procedure . the corpus-based chinese speech synthesis system selects the synthesis unit according to context similarity between target unit and candidate unit . neither prosody parameter prediction nor prosody feature modification is needed . the informal test shows that the synthesized speech is quite natural , and the speaking style of original speaker is preserved because units are all from the speaker 's utterances .
this paper describes our recent work in developing an unified dutch and german speech recognition system in the speechdat domain . the acoustic component of the unified dutch and german speech recognition system is accomplished through sharing common phonemes without preserving any information about the languages . we propose a more robust mce-based training algorithm , where only the language dependent phoneme models are allowed to be adjusted , according to the type of training data . experimental results on dutch and german subword recognition tasks clearly show an overall string error rate reduction of about 7 % and 13 % obtained by the newly trained unified dutch and german speech recognition system in comparison with the conventional mce-trained multilingual system .
